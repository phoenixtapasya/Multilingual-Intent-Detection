{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Multilingual Intent Detection Using Transformer Models**","metadata":{}},{"cell_type":"markdown","source":"#### In an increasingly globalized digital world, the ability of machines to accurately understand user intent across languages is vital to building inclusive and intelligent systems. This project aims to solve the complex challenge of multilingual intent detection using state-of-the-art transformer-based models, focusing on the MASSIVE dataset a benchmark for multilingual SLU (Spoken Language Understanding) covering 51 languages and 60 intent classes.\n\n#### We fine-tuned the XLM-RoBERTa base model, renowned for its cross-lingual capabilities, to classify English utterances by intent. The model was trained on 5,000 filtered and preprocessed English utterances, leveraging Hugging Faceâ€™s Trainer API, optimized with techniques like attention-based tokenization, max-length truncation (64 tokens), and dynamic padding to ensure memory efficiency and stable convergence.\n\n#### The model's performance was evaluated using standard classification metrics:\n\n- Accuracy: 86%\n\n- Weighted F1-score: 0.855\n\n- Evaluation loss: 0.606\n\n- Evaluation throughput: ~487 samples/sec\n\n#### During training, the model demonstrated strong scalability and computational efficiency, with:\n\n- Training throughput: ~92 samples/sec\n\n- Total training FLOPs: 493.6 trillion\n\n- Average training loss: 0.787\n\n#### These results position our solution as both highly accurate and computationally efficient, offering a robust intent detection backbone for multilingual digital assistants, voice interfaces, and smart automation platforms. The use of XLM-RoBERTa, combined with task-specific fine-tuning, ensures both linguistic generalization and domain-specific accuracy, making this project a standout in applied NLP innovation.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nif tf.test.gpu_device_name():\n    print('GPU Found')\n    !nvidia-smi\nelse:\n    print(\"GPU not Found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:42:29.467521Z","iopub.execute_input":"2025-05-19T05:42:29.467815Z","iopub.status.idle":"2025-05-19T05:42:29.668888Z","shell.execute_reply.started":"2025-05-19T05:42:29.467794Z","shell.execute_reply":"2025-05-19T05:42:29.668030Z"}},"outputs":[{"name":"stdout","text":"GPU Found\nMon May 19 05:42:29 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             30W /  250W |     257MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747633349.470304      35 gpu_device.cc:2022] Created device /device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:11:27.128100Z","iopub.execute_input":"2025-05-19T06:11:27.129030Z","iopub.status.idle":"2025-05-19T06:11:30.352079Z","shell.execute_reply.started":"2025-05-19T06:11:27.128996Z","shell.execute_reply":"2025-05-19T06:11:30.351072Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:11:51.495132Z","iopub.execute_input":"2025-05-19T06:11:51.495405Z","iopub.status.idle":"2025-05-19T06:11:51.500116Z","shell.execute_reply.started":"2025-05-19T06:11:51.495382Z","shell.execute_reply":"2025-05-19T06:11:51.499211Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# 2. Load MASSIVE dataset","metadata":{}},{"cell_type":"markdown","source":"### Dataset\n#### MASSIVE (Multilingual Amazon SLURP for Slot-filling, Intent classification, and Virtual assistant Evaluation) is a robust dataset tailored for multilingual intent classification tasks. It comprises:\n- Size: 1 million utterances\n- Languages: 51 typologically diverse languages\n- Domains: 18\n- Intents: 60\n- Slots: 55\n- This dataset is ideal for training and evaluating multilingual models in intent detection tasks.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"AmazonScience/massive\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:18.916992Z","iopub.execute_input":"2025-05-19T06:12:18.917268Z","iopub.status.idle":"2025-05-19T06:12:20.126880Z","shell.execute_reply.started":"2025-05-19T06:12:18.917250Z","shell.execute_reply":"2025-05-19T06:12:20.126134Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# 3. Filter for English samples","metadata":{}},{"cell_type":"code","source":"en_train = dataset['train'].filter(lambda x: x['locale'] == 'en-US').shuffle(seed=42).select(range(5000))\nen_test = dataset['test'].filter(lambda x: x['locale'] == 'en-US').shuffle(seed=42).select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:23.151120Z","iopub.execute_input":"2025-05-19T06:12:23.151389Z","iopub.status.idle":"2025-05-19T06:12:23.182387Z","shell.execute_reply.started":"2025-05-19T06:12:23.151369Z","shell.execute_reply":"2025-05-19T06:12:23.181874Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(en_train.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:57:16.054292Z","iopub.execute_input":"2025-05-19T05:57:16.055054Z","iopub.status.idle":"2025-05-19T05:57:16.059033Z","shell.execute_reply.started":"2025-05-19T05:57:16.055030Z","shell.execute_reply":"2025-05-19T05:57:16.058228Z"}},"outputs":[{"name":"stdout","text":"['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 4. Load tokenizer and model","metadata":{}},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:36.596179Z","iopub.execute_input":"2025-05-19T06:12:36.596491Z","iopub.status.idle":"2025-05-19T06:12:39.534390Z","shell.execute_reply.started":"2025-05-19T06:12:36.596471Z","shell.execute_reply":"2025-05-19T06:12:39.533839Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# 5. Tokenization","metadata":{}},{"cell_type":"code","source":"def preprocess(example):\n    return tokenizer(example['utt'], truncation=True, padding='max_length', max_length=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:14:14.840570Z","iopub.execute_input":"2025-05-19T06:14:14.841316Z","iopub.status.idle":"2025-05-19T06:14:14.844933Z","shell.execute_reply.started":"2025-05-19T06:14:14.841294Z","shell.execute_reply":"2025-05-19T06:14:14.844010Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"train_enc = en_train.map(preprocess, batched=True)\ntest_enc = en_test.map(preprocess, batched=True)\n\ntrain_enc = train_enc.rename_column(\"intent\", \"labels\")\ntest_enc = test_enc.rename_column(\"intent\", \"labels\")\n\ntrain_enc.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_enc.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:15:20.328854Z","iopub.execute_input":"2025-05-19T06:15:20.329398Z","iopub.status.idle":"2025-05-19T06:15:20.386244Z","shell.execute_reply.started":"2025-05-19T06:15:20.329378Z","shell.execute_reply":"2025-05-19T06:15:20.385732Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average='weighted')\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:17:44.777263Z","iopub.execute_input":"2025-05-19T06:17:44.777972Z","iopub.status.idle":"2025-05-19T06:17:44.781746Z","shell.execute_reply.started":"2025-05-19T06:17:44.777948Z","shell.execute_reply":"2025-05-19T06:17:44.781060Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from transformers import TrainingArguments\nprint(TrainingArguments.__module__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:26:34.788830Z","iopub.execute_input":"2025-05-19T06:26:34.789319Z","iopub.status.idle":"2025-05-19T06:26:34.793437Z","shell.execute_reply.started":"2025-05-19T06:26:34.789297Z","shell.execute_reply":"2025-05-19T06:26:34.792557Z"}},"outputs":[{"name":"stdout","text":"transformers.training_args\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# 6. Training Parameters","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=[]  # ðŸ‘ˆ disables W&B logging\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:52:14.644306Z","iopub.execute_input":"2025-05-19T06:52:14.644569Z","iopub.status.idle":"2025-05-19T06:52:14.672013Z","shell.execute_reply.started":"2025-05-19T06:52:14.644551Z","shell.execute_reply":"2025-05-19T06:52:14.671427Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_enc,\n    eval_dataset=test_enc,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:52:19.498709Z","iopub.execute_input":"2025-05-19T06:52:19.498984Z","iopub.status.idle":"2025-05-19T06:52:19.512899Z","shell.execute_reply.started":"2025-05-19T06:52:19.498964Z","shell.execute_reply":"2025-05-19T06:52:19.512225Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1039136796.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:52:22.867552Z","iopub.execute_input":"2025-05-19T06:52:22.868314Z","iopub.status.idle":"2025-05-19T06:55:06.070504Z","shell.execute_reply.started":"2025-05-19T06:52:22.868284Z","shell.execute_reply":"2025-05-19T06:55:06.069421Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [939/939 02:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.884678</td>\n      <td>0.780000</td>\n      <td>0.754111</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.044600</td>\n      <td>0.673678</td>\n      <td>0.835000</td>\n      <td>0.827545</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.044600</td>\n      <td>0.606342</td>\n      <td>0.860000</td>\n      <td>0.855545</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=939, training_loss=0.7874292083187733, metrics={'train_runtime': 162.6467, 'train_samples_per_second': 92.224, 'train_steps_per_second': 5.773, 'total_flos': 493590136320000.0, 'train_loss': 0.7874292083187733, 'epoch': 3.0})"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:55:11.567082Z","iopub.execute_input":"2025-05-19T06:55:11.567812Z","iopub.status.idle":"2025-05-19T06:55:13.631607Z","shell.execute_reply.started":"2025-05-19T06:55:11.567789Z","shell.execute_reply":"2025-05-19T06:55:13.630981Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.6063419580459595, 'eval_accuracy': 0.86, 'eval_f1': 0.8555451589559256, 'eval_runtime': 2.0545, 'eval_samples_per_second': 486.73, 'eval_steps_per_second': 30.664, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"# 7. Resting on Coustom Data","metadata":{}},{"cell_type":"code","source":"def predict_intent(text, model, tokenizer, label_list):\n    device = next(model.parameters()).device\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=64)\n    if \"token_type_ids\" in inputs:\n        inputs.pop(\"token_type_ids\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    predicted_class_id = logits.argmax().item()\n    return label_list[predicted_class_id]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:21:10.057131Z","iopub.execute_input":"2025-05-19T07:21:10.057765Z","iopub.status.idle":"2025-05-19T07:21:10.066509Z","shell.execute_reply.started":"2025-05-19T07:21:10.057742Z","shell.execute_reply":"2025-05-19T07:21:10.065895Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Get label names\nlabel_list = train_enc.features['labels'].names\n\n# Predict on a new test example\ntest_text = \"Reproduz alguma mÃºsica\"\npredicted_intent = predict_intent(test_text, model, tokenizer, label_list)\nprint(f\"Input text: {test_text}\")\nprint(f\"Predicted intent: {predicted_intent}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:31:47.148498Z","iopub.execute_input":"2025-05-19T07:31:47.148798Z","iopub.status.idle":"2025-05-19T07:31:47.166535Z","shell.execute_reply.started":"2025-05-19T07:31:47.148779Z","shell.execute_reply":"2025-05-19T07:31:47.165715Z"}},"outputs":[{"name":"stdout","text":"Input text: Reproduz alguma mÃºsica\nPredicted intent: play_music\n","output_type":"stream"}],"execution_count":73}]}